{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4669e4c-0d64-4016-aec7-3d85fd6d1a04",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d0dbe7-d42b-4f99-81f2-306181f89d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "# import metric\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "# optimization method \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de65793e-e7bb-4c88-815e-96b5abfa01ea",
   "metadata": {},
   "source": [
    "<h2>Alexnet model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295c6811-f208-4777-b213-7d932638143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "# model.fc = nn.Linear(4096, 10)\n",
    "model.classifier._modules['6'] = nn.Linear(4096, 39)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcc2ab-96ff-4e1a-a36c-edc611bb9ff6",
   "metadata": {},
   "source": [
    "<h2>Data Loading and Exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a98844-8c23-412f-b823-37234d16ba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of images for each disease\n",
    "\n",
    "disease_dict = {}\n",
    "\n",
    "for plant_disease in os.listdir('Dataset/'):\n",
    "    disease_dict[plant_disease] = len(os.listdir('Dataset/' + plant_disease))\n",
    "\n",
    "disease_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915732c-618d-4dba-8e20-9d19e3578197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader function\n",
    "\n",
    "def data_loader(path, batch_size):\n",
    "    \n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize(255), transforms.CenterCrop(224), transforms.ToTensor()]\n",
    "    )\n",
    "    \n",
    "    dataset = datasets.ImageFolder(path, transform = transform)\n",
    "    \n",
    "    indices = list(range(len(dataset)))\n",
    "    \n",
    "    split = int(np.floor(0.85 * len(dataset)))\n",
    "    \n",
    "    validation = int(np.floor(0.70 * split))\n",
    "    \n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices, validation_indices, test_indices = (\n",
    "        indices[:validation//16],\n",
    "        indices[validation//16:split//16],\n",
    "        indices[split//16:((split//16)+100)]\n",
    "    )\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=train_sampler\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=test_sampler\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, sampler=validation_sampler\n",
    "    )    \n",
    "    \n",
    "    return dataset, train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8f34f-a5c8-4ec2-937c-f5840f48589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_loader, validation_loader, test_loader = data_loader(path = 'Dataset/', batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b97373-7432-4ae3-93bc-91ac1bbf47fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a704aa-4c29-4d19-8da2-af7d1b4214ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = len(dataset.class_to_idx)\n",
    "print('Number of classes: ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e35a4f3-9f14-4cd6-860f-0551cd2b0d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_dict = dataset.class_to_idx\n",
    "\n",
    "diseases = list(disease_dict.keys())\n",
    "categories = list(disease_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4eabd7-2482-4b02-add1-3a1c4f32c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "device = \"cpu\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5d401-a32c-4244-86df-df01f8e79dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(n_epochs):\n",
    "    \n",
    "    train_loss = []\n",
    "    \n",
    "    for inputs, targets in tqdm(train_loader):\n",
    "        print(inputs.shape)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(inputs)\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        print(train_loss[-1])\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = np.mean(train_loss)\n",
    "    \n",
    "    validation_loss = []\n",
    "    \n",
    "    for inputs, targets in tqdm(validation_loader):\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        output = model(inputs)\n",
    "\n",
    "        loss = criterion(output, targets)\n",
    "\n",
    "        validation_loss.append(loss.item())  # torch to numpy world\n",
    "\n",
    "    validation_loss = np.mean(validation_loss)               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
